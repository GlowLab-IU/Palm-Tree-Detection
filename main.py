# -*- coding: utf-8 -*-
"""
FastAPI application logic for Palm Tree Counting API (Server-side Tile Processing + Filtering)
"""
# --- Imports (gi·ªØ nguy√™n) ---
import io
import os
import logging
import numpy as np
import cv2
import torch
import base64
from datetime import datetime
import math
import requests
from PIL import Image, ImageDraw, ImageOps
from io import BytesIO
from fastapi import FastAPI, HTTPException, Body
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from sahi import AutoDetectionModel
from sahi.predict import get_sliced_prediction
from pydantic import BaseModel, Field
from typing import List, Dict

# --- C·∫•u h√¨nh & H·∫±ng s·ªë (gi·ªØ nguy√™n) ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
MODEL_PATH = os.getenv("MODEL_PATH", "./best_palmtree.pt")
SAHI_SLICE_HEIGHT = int(os.getenv("SAHI_SLICE_HEIGHT", 640))
SAHI_SLICE_WIDTH = int(os.getenv("SAHI_SLICE_WIDTH", 640))
SAHI_OVERLAP_HEIGHT_RATIO = float(os.getenv("SAHI_OVERLAP_HEIGHT_RATIO", 0.2))
SAHI_OVERLAP_WIDTH_RATIO = float(os.getenv("SAHI_OVERLAP_WIDTH_RATIO", 0.2))
SAHI_CONFIDENCE_THRESHOLD = float(os.getenv("SAHI_CONFIDENCE_THRESHOLD", 0.3))
TARGET_ZOOM = 20
TILE_SIZE = 256
GOOGLE_TILE_URL_TEMPLATE = "https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}"
MAX_TILES = 225

# --- H√†m GIS & V·∫Ω ---
def latLngToWorldXY(lat, lng, zoom):
    # --- S·ª¨A L·ªñI C√ö PH√ÅP: B·ªè comment JS ---
    siny = math.sin(math.radians(lat))
    siny = min(max(siny, -0.9999), 0.9999)
    y = math.log((1 + siny) / (1 - siny))
    worldY = (TILE_SIZE * (1 << zoom) * (1 - y / (2 * math.pi))) / 2
    worldX = TILE_SIZE * (1 << zoom) * (lng / 360 + 0.5)
    return {"x": worldX, "y": worldY}

def getTileBoundingBox(points, zoom):
    # --- S·ª¨A L·ªñI C√ö PH√ÅP: B·ªè comment JS ---
    minWorldX, maxWorldX = float('inf'), float('-inf')
    minWorldY, maxWorldY = float('inf'), float('-inf')
    for p in points:
        worldP = latLngToWorldXY(p["latitude"], p["longitude"], zoom)
        minWorldX = min(minWorldX, worldP["x"])
        maxWorldX = max(maxWorldX, worldP["x"])
        minWorldY = min(minWorldY, worldP["y"])
        maxWorldY = max(maxWorldY, worldP["y"])
    worldBounds = {"minWorldX": minWorldX, "maxWorldX": maxWorldX, "minWorldY": minWorldY, "maxWorldY": maxWorldY}
    minTileX = math.floor(worldBounds["minWorldX"] / TILE_SIZE)
    maxTileX = math.floor(worldBounds["maxWorldX"] / TILE_SIZE)
    minTileY = math.floor(worldBounds["minWorldY"] / TILE_SIZE)
    maxTileY = math.floor(worldBounds["maxWorldY"] / TILE_SIZE)
    tileBounds = {"minTileX": minTileX, "maxTileX": maxTileX, "minTileY": minTileY, "maxTileY": maxTileY}
    return tileBounds

def draw_rounded_rectangle(img, pt1, pt2, color, thickness, radius=1):
    # --- S·ª¨A L·ªñI C√ö PH√ÅP: B·ªè comment JS ---
    x1, y1 = int(pt1[0]), int(pt1[1])
    x2, y2 = int(pt2[0]), int(pt2[1])
    if thickness == -1:
        cv2.rectangle(img, (x1 + radius, y1), (x2 - radius, y2), color, -1)
        cv2.rectangle(img, (x1, y1 + radius), (x2, y2 - radius), color, -1)
        cv2.circle(img, (x1 + radius, y1 + radius), radius, color, -1)
        cv2.circle(img, (x2 - radius, y1 + radius), radius, color, -1)
        cv2.circle(img, (x1 + radius, y2 - radius), radius, color, -1)
        cv2.circle(img, (x2 - radius, y2 - radius), radius, color, -1)
    elif thickness > 0:
        cv2.ellipse(img, (x1 + radius, y1 + radius), (radius, radius), 180, 0, 90, color, thickness)
        cv2.ellipse(img, (x2 - radius, y1 + radius), (radius, radius), 270, 0, 90, color, thickness)
        cv2.ellipse(img, (x1 + radius, y2 - radius), (radius, radius), 90, 0, 90, color, thickness)
        cv2.ellipse(img, (x2 - radius, y2 - radius), (radius, radius), 0, 0, 90, color, thickness)
        cv2.line(img, (x1 + radius, y1), (x2 - radius, y1), color, thickness)
        cv2.line(img, (x1 + radius, y2), (x2 - radius, y2), color, thickness)
        cv2.line(img, (x1, y1 + radius), (x1, y2 - radius), color, thickness)
        cv2.line(img, (x2, y1 + radius), (x2, y2 - radius), color, thickness)

# --- T·∫£i Model (gi·ªØ nguy√™n) ---
use_cuda = torch.cuda.is_available()
device = "cuda:0" if use_cuda else "cpu"
logging.info(f"Device: {device} | CUDA: {torch.version.cuda if use_cuda else 'N/A'} | GPU: {torch.cuda.get_device_name(0) if use_cuda else 'N/A'}")
detection_model = None
try:
    if not os.path.exists(MODEL_PATH): raise FileNotFoundError(f"Model not found: {MODEL_PATH}")
    logging.info(f"‚è≥ Loading model from '{MODEL_PATH}' on '{device.upper()}'...")
    detection_model = AutoDetectionModel.from_pretrained(model_type='yolov8', model_path=MODEL_PATH, confidence_threshold=SAHI_CONFIDENCE_THRESHOLD, device=device)
    logging.info("‚úÖ Model loaded successfully!")
except Exception as e:
    logging.error(f"‚ùå Critical error loading model: {e}")

# --- FastAPI App & Input Model (gi·ªØ nguy√™n) ---
app = FastAPI( title="Palm Tree Detection API (Server Tile + Filter)", version="5.1.1") # Bump version
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])
class LatLng(BaseModel): latitude: float; longitude: float
class PredictRequest(BaseModel): polygon: List[LatLng]; center: LatLng | None = None; area_m2: float | None = None; area_ha: float | None = None

@app.get("/")
def read_root(): return {"message": "POST /predict with JSON polygon."}

# --- ENDPOINT /PREDICT (gi·ªØ nguy√™n logic) ---
@app.post("/predict")
async def predict_from_polygon(data: PredictRequest = Body(...)):
    if detection_model is None: raise HTTPException(status_code=503, detail="Error: Model not loaded.")
    try:
        polygon_latlng = [p.dict() for p in data.polygon]
        if len(polygon_latlng) < 3: raise HTTPException(status_code=400, detail="Invalid polygon")
        logging.info(f"Processing polygon with {len(polygon_latlng)} points.")

        # 1. T√≠nh to√°n & T·∫£i Tile
        tileBounds = getTileBoundingBox(polygon_latlng, TARGET_ZOOM)
        minTileX, maxTileX = tileBounds["minTileX"], tileBounds["maxTileX"]
        minTileY, maxTileY = tileBounds["minTileY"], tileBounds["maxTileY"]
        tiles_to_download = [(x, y) for x in range(minTileX, maxTileX + 1) for y in range(minTileY, maxTileY + 1)]
        logging.info(f"Tiles to download: {len(tiles_to_download)}")
        if len(tiles_to_download) > MAX_TILES: raise HTTPException(status_code=400, detail=f"Area too large ({len(tiles_to_download)} tiles). Max {MAX_TILES}.")
        if not tiles_to_download: raise HTTPException(status_code=400, detail="No tiles found")
        stitch_width = (maxTileX - minTileX + 1) * TILE_SIZE
        stitch_height = (maxTileY - minTileY + 1) * TILE_SIZE
        stitched_image = Image.new("RGBA", (stitch_width, stitch_height))
        stitchOriginWorldX = minTileX * TILE_SIZE
        stitchOriginWorldY = minTileY * TILE_SIZE
        session = requests.Session()
        logging.info("Downloading and stitching tiles...")
        for x, y in tiles_to_download:
            url = GOOGLE_TILE_URL_TEMPLATE.format(x=x, y=y, z=TARGET_ZOOM)
            try:
                response = session.get(url, timeout=10); response.raise_for_status()
                tile_image = Image.open(io.BytesIO(response.content)).convert("RGBA")
                stitched_image.paste(tile_image, ((x - minTileX) * TILE_SIZE, (y - minTileY) * TILE_SIZE))
            except requests.exceptions.RequestException as e:
                logging.warning(f"Failed to download tile ({x},{y}): {e}. Filling black.")
                draw = ImageDraw.Draw(stitched_image)
                offsetX, offsetY = (x - minTileX) * TILE_SIZE, (y - minTileY) * TILE_SIZE
                draw.rectangle([(offsetX, offsetY), (offsetX + TILE_SIZE, offsetY + TILE_SIZE)], fill=(0, 0, 0, 255))
        logging.info("Stitching complete.")

        # 2. Chuy·ªÉn ·∫£nh gh√©p sang NumPy RGB cho SAHI
        stitched_image_rgb_pil = stitched_image.convert("RGB")
        stitched_image_rgb_np = np.array(stitched_image_rgb_pil)
        logging.info(f"Stitched image prepared for SAHI, shape: {stitched_image_rgb_np.shape}")

        # 3. CH·∫†Y SAHI TR√äN TO√ÄN B·ªò ·∫¢NH GH√âP
        logging.info("Starting SAHI prediction on stitched image...")
        result = get_sliced_prediction(
            stitched_image_rgb_np,
            detection_model,
            slice_height=SAHI_SLICE_HEIGHT, slice_width=SAHI_SLICE_WIDTH,
            overlap_height_ratio=SAHI_OVERLAP_HEIGHT_RATIO,
            overlap_width_ratio=SAHI_OVERLAP_WIDTH_RATIO
        )
        all_predictions = result.object_prediction_list
        logging.info(f"üå¥ SAHI found {len(all_predictions)} total objects on stitched image.")

        # 4. T·∫°o m·∫∑t n·∫° Polygon tr√™n k√≠ch th∆∞·ªõc ·∫£nh gh√©p
        polygon_px = []
        for p in polygon_latlng:
            worldP = latLngToWorldXY(p["latitude"], p["longitude"], TARGET_ZOOM)
            x = worldP["x"] - stitchOriginWorldX
            y = worldP["y"] - stitchOriginWorldY
            polygon_px.append((x, y))

        mask_pil = Image.new("L", (stitch_width, stitch_height), 0)
        ImageDraw.Draw(mask_pil).polygon(polygon_px, outline=1, fill=1)
        mask_np = np.array(mask_pil)

        # 5. L·ªåC C√ÅC D·ª∞ ƒêO√ÅN N·∫∞M TRONG POLYGON MASK
        filtered_predictions = []
        for pred in all_predictions:
            box = pred.bbox
            center_x = int((box.minx + box.maxx) / 2)
            center_y = int((box.miny + box.maxy) / 2)
            if 0 <= center_y < mask_np.shape[0] and 0 <= center_x < mask_np.shape[1] and mask_np[center_y, center_x] > 0:
                filtered_predictions.append({
                    "class_id": pred.category.id,
                    "class_name": pred.category.name,
                    "confidence": round(float(pred.score.value), 4),
                    "box_pixels_stitched": [int(box.minx), int(box.miny), int(box.maxx), int(box.maxy)]
                })
        logging.info(f"‚úÖ Filtered down to {len(filtered_predictions)} objects inside the polygon.")
        predicted_count = len(filtered_predictions)

        # 6. C·∫Øt ·∫£nh g·ªëc (RGBA) theo bounding box c·ªßa polygon ƒë·ªÉ g·ª≠i v·ªÅ client
        logging.info("Cropping original stitched image for client...")
        bbox_mask = mask_pil.getbbox()
        if not bbox_mask: raise HTTPException(status_code=400, detail="Polygon outside tile area")
        cropped_image_rgba = stitched_image.crop(bbox_mask)

        # M√£ h√≥a ·∫£nh c·∫Øt g·ªëc (input) sang Base64 JPEG
        input_image_rgb = cropped_image_rgba.convert("RGB")
        buffered_input = BytesIO()
        input_image_rgb.save(buffered_input, format="JPEG", quality=90)
        input_image_base64 = base64.b64encode(buffered_input.getvalue()).decode('utf-8')

        # 7. V·∫Ω overlay: V·∫Ω c√°c box ƒê√É L·ªåC l√™n ·∫£nh ƒê√É C·∫ÆT
        image_overlay_bgra = cv2.cvtColor(np.array(cropped_image_rgba), cv2.COLOR_RGBA2BGRA)
        crop_origin_x, crop_origin_y = bbox_mask[0], bbox_mask[1]

        for tree_data in filtered_predictions:
            box_stitched = tree_data['box_pixels_stitched']
            x_min = box_stitched[0] - crop_origin_x
            y_min = box_stitched[1] - crop_origin_y
            x_max = box_stitched[2] - crop_origin_x
            y_max = box_stitched[3] - crop_origin_y
            label = f"{tree_data['class_name']}"
            color = (0, 125, 255); thickness = 2
            font_scale = 0.6; font = cv2.FONT_HERSHEY_SIMPLEX
            draw_rounded_rectangle(image_overlay_bgra, (x_min, y_min), (x_max, y_max), color, thickness, radius=20)
            (text_width, text_height), _ = cv2.getTextSize(label, font, font_scale, thickness)
            label_bg_ymin = max(y_min + text_height + 10, text_height + 10)
            label_bg_xmin = max(x_min, 0)
            draw_rounded_rectangle(image_overlay_bgra, (label_bg_xmin, label_bg_ymin - text_height - 10), (label_bg_xmin + text_width + 10, label_bg_ymin), color, -1, radius=5)
            cv2.putText(image_overlay_bgra, label, (label_bg_xmin + 5, label_bg_ymin - 5), font, font_scale, (255, 255, 255), 1, cv2.LINE_AA)

        # 8. M√£ h√≥a ·∫£nh overlay (BGRA) sang Base64 JPEG
        image_overlay_bgr_final = cv2.cvtColor(image_overlay_bgra, cv2.COLOR_BGRA2BGR)
        is_success, buffer = cv2.imencode(".jpg", image_overlay_bgr_final, [cv2.IMWRITE_JPEG_QUALITY, 85])
        if not is_success: raise HTTPException(status_code=500, detail="Cannot encode overlay image.")
        overlay_image_base64 = base64.b64encode(buffer).decode("utf-8")
        logging.info("Overlay Base64 image created successfully.")

        # 9. Tr·∫£ k·∫øt qu·∫£ v·ªÅ app (Th√™m input_image_base64)
        return JSONResponse(content={
            "predicted_count": predicted_count,
            "input_image_base64": input_image_base64,
            "overlay_image_base64": overlay_image_base64
        })

    except HTTPException as http_exc: raise http_exc
    except Exception as e:
        logging.error(f"Critical error in /predict: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Server error: {str(e)}")

